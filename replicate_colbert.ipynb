{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "086fb744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6d8e5",
   "metadata": {},
   "source": [
    "### Check if data has the same distribution as their final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18615f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('humor-detection/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6e35ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    100000\n",
       "True     100000\n",
       "Name: humor, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# has the right amount of jokes & non-jokes\n",
    "data.humor.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fad6482",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Me: my cat isn't overweight; she's just big-boned vet: this is a dog</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Did you hear, john wayne bobbit got his penis cut off again? isn't that redickless?</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jesus walks into a bar no he didn't, because he isn't real.</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Video breaks down why machismo isn't synonymous with latino men</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George clooney's ex-anchorman dad isn't having it with sinclair</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miss russia 'haters' say elmira abdrazakova isn't russian enough (photos)</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>It's so cool how math isn't real now that i'm a grown up.</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>There's a shockingly high chance the seafood you're eating isn't legit</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This is what happens 'when mama isn't home'</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wells fargo doesn't want you to know its scandal isn't hurting profits</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>924 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    humor\n",
       "text                                                     \n",
       "Me: my cat isn't overweight; she's just big-bon...   True\n",
       "Did you hear, john wayne bobbit got his penis c...   True\n",
       "Jesus walks into a bar no he didn't, because he...   True\n",
       "Video breaks down why machismo isn't synonymous...  False\n",
       "George clooney's ex-anchorman dad isn't having ...  False\n",
       "...                                                   ...\n",
       "Miss russia 'haters' say elmira abdrazakova isn...  False\n",
       "It's so cool how math isn't real now that i'm a...   True\n",
       "There's a shockingly high chance the seafood yo...  False\n",
       "This is what happens 'when mama isn't home'         False\n",
       "Wells fargo doesn't want you to know its scanda...  False\n",
       "\n",
       "[924 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# haven't done processing for special words (eg isn't --> is not)\n",
    "data[data['text'].str.contains(\"isn't\")].set_index('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31bb223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ' '.join(data['text'])\n",
    "all_puncs = re.findall(r'[^\\w\\s]', all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3145244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_puncs = []\n",
    "for t in data['text']:\n",
    "    num_puncs = re.findall(r'[^\\w\\s]', t)\n",
    "    all_puncs.append(num_puncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a3e4cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_puncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abbb3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi I am is not'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we need to expand contractions ourselves\n",
    "\n",
    "#!pip install contractions\n",
    "import contractions as ct\n",
    "ct.fix(\"Hi I'm isn't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e949be",
   "metadata": {},
   "source": [
    "## ColBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "#!pip install --upgrade --user transformers==3.0.2 # the authors probably used version 3.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b57749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "# import bert_tokenization as tokenization\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras \n",
    "\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from transformers import *\n",
    "from transformers import TFBertModel\n",
    "\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re    #for regex\n",
    "\n",
    "# import keras model layers\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Concatenate\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f977d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENT_INPUT_LEN = 20\n",
    "DOC_INPUT_LEN = 100\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "# 18 inputs, 3 for each parallel path (5 sentence-level paths & 1 document-level path)\n",
    "input_sent1_1 = Input(shape=(SENT_INPUT_LEN,), name='input_ii_sent1') # input IDs\n",
    "input_sent1_2 = Input(shape=(SENT_INPUT_LEN,), name='input_am_sent1') # attention masks\n",
    "input_sent1_3 = Input(shape=(SENT_INPUT_LEN,), name='input_tti_sent1') # token type IDs\n",
    "\n",
    "input_sent2_1 = Input(shape=(SENT_INPUT_LEN,), name='input_ii_sent2')\n",
    "input_sent2_2 = Input(shape=(SENT_INPUT_LEN,), name='input_am_sent2')\n",
    "input_sent2_3 = Input(shape=(SENT_INPUT_LEN,), name='input_tti_sent2')\n",
    "\n",
    "input_sent3_1 = Input(shape=(SENT_INPUT_LEN,), name='input_ii_sent3')\n",
    "input_sent3_2 = Input(shape=(SENT_INPUT_LEN,), name='input_am_sent3')\n",
    "input_sent3_3 = Input(shape=(SENT_INPUT_LEN,), name='input_tti_sent3')\n",
    "\n",
    "input_sent4_1 = Input(shape=(SENT_INPUT_LEN,), name='input_ii_sent4')\n",
    "input_sent4_2 = Input(shape=(SENT_INPUT_LEN,), name='input_am_sent4')\n",
    "input_sent4_3 = Input(shape=(SENT_INPUT_LEN,), name='input_tti_sent4')\n",
    "\n",
    "input_sent5_1 = Input(shape=(SENT_INPUT_LEN,), name='input_ii_sent5')\n",
    "input_sent5_2 = Input(shape=(SENT_INPUT_LEN,), name='input_am_sent5')\n",
    "input_sent5_3 = Input(shape=(SENT_INPUT_LEN,), name='input_tti_sent5')\n",
    "\n",
    "input_doc_1 = Input(shape=(DOC_INPUT_LEN,), name='input_ii_doc')\n",
    "input_doc_2 = Input(shape=(DOC_INPUT_LEN,), name='input_am_doc')\n",
    "input_doc_3 = Input(shape=(DOC_INPUT_LEN,), name='input_tti_doc')\n",
    "\n",
    "# embedding layer for sentences and documents\n",
    "#bert_embeddings = Embedding(num_tokens,embedding_dim,embeddings_initializer=keras.initializers.Constant(embedding_matrix),trainable=False)\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "bert_embeddings = bert_model(input_ids=input_sent1_1, attention_mask=input_sent1_2, token_type_ids=input_sent1_3)\n",
    "\n",
    "# get pooled vectors of BERT sentence embeddings\n",
    "x1 = bert_embeddings[0]['pooled_vector'] # can also do GlobalAveragePooling1D()\n",
    "x2 = bert_embeddings[1]['pooled_vector']\n",
    "x3 = bert_embeddings[2]['pooled_vector']\n",
    "x4 = bert_embeddings[3]['pooled_vector']\n",
    "x5 = bert_embeddings[4]['pooled_vector']\n",
    "x6 = bert_embeddings[5]['pooled_vector']\n",
    "\n",
    "# fully connected layer w/ dropout\n",
    "h1_1 = Dense(32, activation='relu', name=\"hidden1_sent1\")(x1)\n",
    "h1_2 = Dense(32, activation='relu', name=\"hidden1_sent2\")(x2)\n",
    "h1_3 = Dense(32, activation='relu', name=\"hidden1_sent3\")(x3)\n",
    "h1_4 = Dense(32, activation='relu', name=\"hidden1_sent4\")(x4)\n",
    "h1_5 = Dense(32, activation='relu', name=\"hidden1_sent5\")(x5)\n",
    "h1_6 = Dense(256, activation='relu', name=\"hidden1_doc\")(x6)\n",
    "\n",
    "h1_dropout1 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent1\")(h1_1) ####################################################\n",
    "h1_dropout2 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent2\")(h1_2) ####################################################\n",
    "h1_dropout3 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent3\")(h1_3) #                rate TO BE CHANGED                # \n",
    "h1_dropout4 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent4\")(h1_4) #                                                  #\n",
    "h1_dropout5 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent5\")(h1_5) ####################################################\n",
    "h1_dropout6 = Dropout(DROPOUT_RATE, name=\"h1_dropout_doc\")(h1_6)   ####################################################\n",
    "\n",
    "# fully connected layer\n",
    "h2_1 = Dense(8, activation='relu', name=\"hidden2_sent1\")(h1_dropout1)\n",
    "h2_2 = Dense(8, activation='relu', name=\"hidden2_sent2\")(h1_dropout2)\n",
    "h2_3 = Dense(8, activation='relu', name=\"hidden2_sent3\")(h1_dropout3)\n",
    "h2_4 = Dense(8, activation='relu', name=\"hidden2_sent4\")(h1_dropout4)\n",
    "h2_5 = Dense(8, activation='relu', name=\"hidden2_sent5\")(h1_dropout5)\n",
    "h2_6 = Dense(64, activation='relu', name=\"hidden2_doc\")(h1_dropout6)\n",
    "\n",
    "# concatenate outputs of all 6 parallel layers\n",
    "xx = Concatenate()([h2_1, h2_2, h2_3, h2_4, h2_5, h2_6])\n",
    "\n",
    "# fully connected layer w/ dropout for concatenated inputs\n",
    "h3 = Dense(512, activation='relu', name=\"hidden3\")(xx)\n",
    "h3_dropout = Dropout(DROPOUT_RATE)(h3) ################ rate TO BE CHANGED ################\n",
    "\n",
    "# fully connected layer\n",
    "h4 = Dense(256, activation='relu', name=\"hidden4\")(h3_dropout)\n",
    "\n",
    "# final output layer\n",
    "yhat = Dense(1, activation='sigmoid', name=\"output\")(h4) # need to figure out dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8fbb3",
   "metadata": {},
   "source": [
    "notes\n",
    "- not sure if we can just use the default axis for Concatenate (default is axis=-1)\n",
    "- dropout rate is not specified (also should different dropout layers have different dropout rates?)\n",
    "- output layer activation function is sigmoid right???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=INPUT, outputs=yhat, name=\"keras_func_model\")\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics = ['sparse_categorical_accuracy']) # TO BE CHANGED"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

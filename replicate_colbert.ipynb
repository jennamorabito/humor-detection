{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "086fb744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6d8e5",
   "metadata": {},
   "source": [
    "### Check if data has the same distribution as their final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18615f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba6e35ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    100000\n",
       "True     100000\n",
       "Name: humor, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# has the right amount of jokes & non-jokes\n",
    "data.humor.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5e0f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test data\n",
    "x_train, x_test = data['text'][:160000], data['text'][160000:]\n",
    "y_train, y_test = data['humor'][:160000], data['humor'][160000:]\n",
    "\n",
    "# cast back into dataframes \n",
    "x_train = x_train.to_frame('text')\n",
    "x_test = x_test.to_frame('text')\n",
    "y_train = y_train.to_frame('humor')\n",
    "y_test = y_test.to_frame('humor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8fad6482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Me: my cat isn't overweight; she's just big-boned vet: this is a dog</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Did you hear, john wayne bobbit got his penis cut off again? isn't that redickless?</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jesus walks into a bar no he didn't, because he isn't real.</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Video breaks down why machismo isn't synonymous with latino men</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George clooney's ex-anchorman dad isn't having it with sinclair</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miss russia 'haters' say elmira abdrazakova isn't russian enough (photos)</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>It's so cool how math isn't real now that i'm a grown up.</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>There's a shockingly high chance the seafood you're eating isn't legit</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This is what happens 'when mama isn't home'</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wells fargo doesn't want you to know its scandal isn't hurting profits</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>924 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    humor\n",
       "text                                                     \n",
       "Me: my cat isn't overweight; she's just big-bon...   True\n",
       "Did you hear, john wayne bobbit got his penis c...   True\n",
       "Jesus walks into a bar no he didn't, because he...   True\n",
       "Video breaks down why machismo isn't synonymous...  False\n",
       "George clooney's ex-anchorman dad isn't having ...  False\n",
       "...                                                   ...\n",
       "Miss russia 'haters' say elmira abdrazakova isn...  False\n",
       "It's so cool how math isn't real now that i'm a...   True\n",
       "There's a shockingly high chance the seafood yo...  False\n",
       "This is what happens 'when mama isn't home'         False\n",
       "Wells fargo doesn't want you to know its scanda...  False\n",
       "\n",
       "[924 rows x 1 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# haven't done processing for special words (eg isn't --> is not)\n",
    "data[data['text'].str.contains(\"isn't\")].set_index('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31bb223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ' '.join(data['text'])\n",
    "all_puncs = re.findall(r'[^\\w\\s]', all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3145244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_puncs = []\n",
    "for t in data['text']:\n",
    "    num_puncs = re.findall(r'[^\\w\\s]', t)\n",
    "    all_puncs.append(num_puncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a3e4cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_puncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abbb3c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-382f69dcab46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#!pip install contractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hi I'm isn't\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "# if we need to expand contractions ourselves\n",
    "\n",
    "#!pip install contractions\n",
    "import contractions as ct\n",
    "ct.fix(\"Hi I'm isn't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e949be",
   "metadata": {},
   "source": [
    "## ColBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "241e8398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\meerw\\appdata\\roaming\\python\\python37\\site-packages (3.0.2)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: filelock in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: requests in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: numpy in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in c:\\users\\meerw\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\users\\meerw\\appdata\\roaming\\python\\python37\\site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: click in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: transformers==3.0.2 in c:\\users\\meerw\\appdata\\roaming\\python\\python37\\site-packages (3.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (2021.4.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in c:\\users\\meerw\\appdata\\roaming\\python\\python37\\site-packages (from transformers==3.0.2) (0.1.91)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in c:\\users\\meerw\\appdata\\roaming\\python\\python37\\site-packages (from transformers==3.0.2) (0.8.1rc1)\n",
      "Requirement already satisfied: filelock in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (3.0.12)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (0.0.47)\n",
      "Requirement already satisfied: packaging in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (20.9)\n",
      "Requirement already satisfied: numpy in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (1.21.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (4.59.0)\n",
      "Requirement already satisfied: requests in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from transformers==3.0.2) (2.25.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from packaging->transformers==3.0.2) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from requests->transformers==3.0.2) (1.26.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.2) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
      "Requirement already satisfied: six in c:\\users\\meerw\\anaconda3\\lib\\site-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "!pip install transformers\n",
    "# the authors probably used version 3.0.2\n",
    "!pip install --upgrade --user transformers==3.0.2 \n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98b57749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "4.17.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "# import bert_tokenization as tokenization\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras \n",
    "\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "# from transformers import *\n",
    "import transformers\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re    #for regex\n",
    "\n",
    "# import keras model layers\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Concatenate\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(tf.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0f977d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "SENT_INPUT_LEN = 20\n",
    "DOC_INPUT_LEN = 100\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "# 18 inputs, 3 for each parallel path (5 sentence-level paths & 1 document-level path)\n",
    "input_sent1_1 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_ii_sent1') # input IDs\n",
    "input_sent1_2 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_am_sent1') # attention masks\n",
    "input_sent1_3 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_tti_sent1') # token type IDs\n",
    "\n",
    "input_sent2_1 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_ii_sent2')\n",
    "input_sent2_2 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_am_sent2')\n",
    "input_sent2_3 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_tti_sent2')\n",
    "\n",
    "input_sent3_1 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_ii_sent3')\n",
    "input_sent3_2 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_am_sent3')\n",
    "input_sent3_3 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_tti_sent3')\n",
    "\n",
    "input_sent4_1 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_ii_sent4')\n",
    "input_sent4_2 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_am_sent4')\n",
    "input_sent4_3 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_tti_sent4')\n",
    "\n",
    "input_sent5_1 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_ii_sent5')\n",
    "input_sent5_2 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_am_sent5')\n",
    "input_sent5_3 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_tti_sent5')\n",
    "\n",
    "input_doc_1 = Input(shape=(DOC_INPUT_LEN,), dtype=tf.int32, name='input_ii_doc')\n",
    "input_doc_2 = Input(shape=(DOC_INPUT_LEN,), dtype=tf.int32, name='input_am_doc')\n",
    "input_doc_3 = Input(shape=(DOC_INPUT_LEN,), dtype=tf.int32, name='input_tti_doc')\n",
    "\n",
    "# embedding layer for sentences and documents\n",
    "#bert_embeddings = Embedding(num_tokens,embedding_dim,embeddings_initializer=keras.initializers.Constant(embedding_matrix),trainable=False)\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "#bert_embeddings = bert_model(input_ids=input_sent1_1, attention_mask=input_sent1_2, token_type_ids=input_sent1_3) ########### HELP ###########\n",
    "bert_embeddings1 = bert_model(input_sent1_1, attention_mask=input_sent1_2, token_type_ids=input_sent1_3)\n",
    "bert_embeddings2 = bert_model(input_sent2_1, attention_mask=input_sent2_2, token_type_ids=input_sent2_3)\n",
    "bert_embeddings3 = bert_model(input_sent3_1, attention_mask=input_sent3_2, token_type_ids=input_sent3_3)\n",
    "bert_embeddings4 = bert_model(input_sent4_1, attention_mask=input_sent4_2, token_type_ids=input_sent4_3)\n",
    "bert_embeddings5 = bert_model(input_sent5_1, attention_mask=input_sent5_2, token_type_ids=input_sent5_3)\n",
    "bert_embeddings6 = bert_model(input_doc_1, attention_mask=input_doc_2, token_type_ids=input_doc_3)\n",
    "\n",
    "# get pooled vectors of BERT sentence embeddings\n",
    "#x1 = bert_embeddings[0]['pooled_vector'] # can also do GlobalAveragePooling1D()\n",
    "#x2 = bert_embeddings[1]['pooled_vector']\n",
    "#x3 = bert_embeddings[2]['pooled_vector']\n",
    "#x4 = bert_embeddings[3]['pooled_vector']\n",
    "#x5 = bert_embeddings[4]['pooled_vector']\n",
    "#x6 = bert_embeddings[5]['pooled_vector']\n",
    "x1 = bert_embeddings1[1] \n",
    "x2 = bert_embeddings2[1] \n",
    "x3 = bert_embeddings3[1]\n",
    "x4 = bert_embeddings4[1]\n",
    "x5 = bert_embeddings5[1]\n",
    "x6 = bert_embeddings6[1] \n",
    "\n",
    "# fully connected layer w/ dropout\n",
    "h1_1 = Dense(32, activation='relu', name=\"hidden1_sent1\")(x1)\n",
    "h1_2 = Dense(32, activation='relu', name=\"hidden1_sent2\")(x2)\n",
    "h1_3 = Dense(32, activation='relu', name=\"hidden1_sent3\")(x3)\n",
    "h1_4 = Dense(32, activation='relu', name=\"hidden1_sent4\")(x4)\n",
    "h1_5 = Dense(32, activation='relu', name=\"hidden1_sent5\")(x5)\n",
    "h1_6 = Dense(256, activation='relu', name=\"hidden1_doc\")(x6)\n",
    "\n",
    "h1_dropout1 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent1\")(h1_1) ####################################################\n",
    "h1_dropout2 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent2\")(h1_2) ####################################################\n",
    "h1_dropout3 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent3\")(h1_3) #                rate TO BE CHANGED                # \n",
    "h1_dropout4 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent4\")(h1_4) #                                                  #\n",
    "h1_dropout5 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent5\")(h1_5) ####################################################\n",
    "h1_dropout6 = Dropout(DROPOUT_RATE, name=\"h1_dropout_doc\")(h1_6)   ####################################################\n",
    "\n",
    "# fully connected layer\n",
    "h2_1 = Dense(8, activation='relu', name=\"hidden2_sent1\")(h1_dropout1)\n",
    "h2_2 = Dense(8, activation='relu', name=\"hidden2_sent2\")(h1_dropout2)\n",
    "h2_3 = Dense(8, activation='relu', name=\"hidden2_sent3\")(h1_dropout3)\n",
    "h2_4 = Dense(8, activation='relu', name=\"hidden2_sent4\")(h1_dropout4)\n",
    "h2_5 = Dense(8, activation='relu', name=\"hidden2_sent5\")(h1_dropout5)\n",
    "h2_6 = Dense(64, activation='relu', name=\"hidden2_doc\")(h1_dropout6)\n",
    "\n",
    "# concatenate outputs of all 6 parallel layers\n",
    "xx = Concatenate()([h2_1, h2_2, h2_3, h2_4, h2_5, h2_6])\n",
    "\n",
    "# fully connected layer w/ dropout for concatenated inputs\n",
    "h3 = Dense(512, activation='relu', name=\"hidden3\")(xx)\n",
    "h3_dropout = Dropout(DROPOUT_RATE)(h3) ################ rate TO BE CHANGED ################\n",
    "\n",
    "# fully connected layer\n",
    "h4 = Dense(256, activation='relu', name=\"hidden4\")(h3_dropout)\n",
    "\n",
    "# final output layer\n",
    "yhat = Dense(1, activation='sigmoid', name=\"output\")(h4) # need to figure out dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b444c0f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe biden rules out 2020 bid: 'guys, i'm not r...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch: darvish gave hitter whiplash with slow ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you call a turtle without its shell? d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  Joe biden rules out 2020 bid: 'guys, i'm not r...  False\n",
       "1  Watch: darvish gave hitter whiplash with slow ...  False\n",
       "2  What do you call a turtle without its shell? d...   True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What kind of cat should you take into the  des...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remember when people used to have to be in sha...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pizza is always good. - everyone we'll see abo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  What kind of cat should you take into the  des...   True\n",
       "1  Remember when people used to have to be in sha...   True\n",
       "2  Pizza is always good. - everyone we'll see abo...   True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df = pd.read_csv('/kaggle/input/200k-short-texts-for-humor-detection/dataset.csv')\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "#df_train = pd.read_csv('/kaggle/input/200k-short-texts-for-humor-detection/train.csv')\n",
    "df_train = pd.read_csv('train.csv')\n",
    "display(df_train.head(3))\n",
    "df_train = df_train[:50]\n",
    "\n",
    "#df_test = pd.read_csv('/kaggle/input/200k-short-texts-for-humor-detection/dev.csv')\n",
    "df_test = pd.read_csv('dev.csv')\n",
    "display(df_test.head(3))\n",
    "df_test = df_test[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57bdebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:31, 31.07s/it]\u001b[A\n",
      "2it [01:01, 30.92s/it]\u001b[A\n",
      "3it [01:32, 30.95s/it]\u001b[A\n",
      "4it [02:03, 30.98s/it]\u001b[A\n",
      "5it [02:35, 31.03s/it]\u001b[A\n",
      "6it [03:06, 31.08s/it]\u001b[A\n",
      "7it [03:37, 31.01s/it]\u001b[A\n",
      "8it [04:08, 31.01s/it]\u001b[A\n",
      "9it [04:38, 30.90s/it]\u001b[A\n",
      "10it [05:09, 30.81s/it]\u001b[A\n",
      "11it [05:40, 30.92s/it]\u001b[A\n",
      "12it [06:11, 30.88s/it]\u001b[A\n",
      "13it [06:43, 31.19s/it]\u001b[A\n",
      "14it [07:14, 31.09s/it]\u001b[A\n",
      "15it [07:44, 30.96s/it]\u001b[A\n",
      "16it [08:15, 30.91s/it]\u001b[A\n",
      "17it [08:46, 30.91s/it]\u001b[A\n",
      "18it [09:17, 31.03s/it]\u001b[A\n",
      "19it [09:49, 31.20s/it]\u001b[A\n",
      "20it [10:20, 31.08s/it]\u001b[A\n",
      "21it [10:51, 31.08s/it]\u001b[A\n",
      "22it [11:21, 30.97s/it]\u001b[A\n",
      "23it [11:53, 31.02s/it]\u001b[A\n",
      "24it [12:23, 30.98s/it]\u001b[A\n",
      "25it [12:55, 31.12s/it]\u001b[A\n",
      "26it [13:26, 31.03s/it]\u001b[A\n",
      "27it [13:57, 31.03s/it]\u001b[A\n",
      "28it [14:28, 30.99s/it]\u001b[A\n",
      "29it [14:59, 31.12s/it]\u001b[A\n",
      "30it [15:30, 31.03s/it]\u001b[A\n",
      "31it [16:01, 30.94s/it]\u001b[A\n",
      "32it [16:32, 30.98s/it]\u001b[A\n",
      "33it [17:03, 31.15s/it]\u001b[A\n",
      "34it [17:34, 31.14s/it]\u001b[A\n",
      "35it [18:07, 31.50s/it]\u001b[A\n",
      "36it [18:38, 31.41s/it]\u001b[A\n",
      "37it [19:09, 31.35s/it]\u001b[A\n",
      "38it [19:40, 31.32s/it]\u001b[A\n",
      "39it [20:12, 31.43s/it]\u001b[A\n",
      "40it [20:43, 31.26s/it]\u001b[A\n",
      "41it [21:14, 31.16s/it]\u001b[A\n",
      "42it [21:45, 31.14s/it]\u001b[A\n",
      "43it [22:16, 31.05s/it]\u001b[A\n",
      "44it [22:47, 31.11s/it]\u001b[A\n",
      "45it [23:19, 31.25s/it]\u001b[A\n",
      "46it [23:50, 31.28s/it]\u001b[A\n",
      "47it [24:21, 31.26s/it]\u001b[A\n",
      "48it [24:52, 31.05s/it]\u001b[A\n",
      "49it [25:23, 31.02s/it]\u001b[A\n",
      "50it [25:54, 31.08s/it]\u001b[A\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:31, 31.05s/it]\u001b[A\n",
      "2it [01:02, 31.55s/it]\u001b[A\n",
      "3it [01:34, 31.37s/it]\u001b[A\n",
      "4it [02:05, 31.27s/it]\u001b[A\n",
      "5it [02:36, 31.16s/it]\u001b[A\n",
      "6it [03:07, 31.07s/it]\u001b[A\n",
      "7it [03:38, 31.18s/it]\u001b[A\n",
      "8it [04:09, 31.26s/it]\u001b[A\n",
      "9it [04:42, 31.53s/it]\u001b[A\n",
      "10it [05:13, 31.44s/it]\u001b[A\n",
      "11it [05:43, 31.21s/it]\u001b[A\n",
      "12it [06:15, 31.27s/it]\u001b[A\n",
      "13it [06:46, 31.13s/it]\u001b[A\n",
      "14it [07:17, 31.25s/it]\u001b[A\n",
      "15it [07:48, 31.13s/it]\u001b[A\n",
      "16it [08:19, 31.11s/it]\u001b[A\n",
      "17it [08:50, 31.05s/it]\u001b[A\n",
      "18it [09:21, 30.93s/it]\u001b[A\n",
      "19it [09:51, 30.89s/it]\u001b[A\n",
      "20it [10:22, 30.86s/it]\u001b[A\n",
      "21it [10:54, 31.12s/it]\u001b[A\n",
      "22it [11:25, 31.08s/it]\u001b[A\n",
      "23it [11:56, 30.94s/it]\u001b[A\n",
      "24it [12:26, 30.76s/it]\u001b[A\n",
      "25it [12:57, 30.74s/it]\u001b[A\n",
      "26it [13:27, 30.75s/it]\u001b[A\n",
      "27it [13:58, 30.77s/it]\u001b[A\n",
      "28it [14:29, 30.75s/it]\u001b[A\n",
      "29it [15:00, 30.76s/it]\u001b[A\n",
      "30it [15:31, 30.81s/it]\u001b[A\n",
      "31it [16:01, 30.75s/it]\u001b[A\n",
      "32it [16:32, 30.71s/it]\u001b[A\n",
      "33it [17:03, 30.73s/it]\u001b[A\n",
      "34it [17:34, 30.80s/it]\u001b[A\n",
      "35it [18:04, 30.80s/it]\u001b[A\n",
      "36it [18:35, 30.80s/it]\u001b[A\n",
      "37it [19:06, 30.81s/it]\u001b[A\n",
      "38it [19:37, 30.85s/it]\u001b[A\n",
      "39it [20:08, 30.79s/it]\u001b[A\n",
      "40it [20:39, 30.88s/it]\u001b[A\n",
      "41it [21:11, 31.31s/it]\u001b[A\n",
      "42it [21:42, 31.20s/it]\u001b[A\n",
      "43it [22:13, 31.03s/it]\u001b[A\n",
      "44it [22:44, 31.04s/it]\u001b[A\n",
      "45it [23:15, 31.04s/it]\u001b[A\n",
      "46it [23:46, 31.00s/it]\u001b[A\n",
      "47it [24:16, 30.94s/it]\u001b[A\n",
      "48it [24:47, 30.90s/it]\u001b[A\n",
      "49it [25:18, 30.86s/it]\u001b[A\n",
      "50it [25:49, 30.99s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from bertembeddings import compute_input_arrays\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "inputs      = compute_input_arrays(df_train, ['text'], tokenizer)\n",
    "test_inputs = compute_input_arrays(df_test, ['text'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8151bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4178it [30:50:28,  7.29s/it]   "
     ]
    }
   ],
   "source": [
    "from bertembeddings import compute_input_arrays\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#inputs      = compute_input_arrays(x_train, ['text'], tokenizer)\n",
    "#test_inputs = compute_input_arrays(x_test, ['text'], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00300e17",
   "metadata": {},
   "source": [
    "### each sentence-level path is differentiated sentence position in the document (maximum 5 sentences in a document)\n",
    "input_sent1_1 = inputs[0] # input ids\n",
    "\n",
    "input_sent1_2 = inputs[1] # attention masks\n",
    "\n",
    "input_sent1_3 = inputs[2] # token type ids (of the first sentences)\n",
    "\n",
    "input_sent2_1 = inputs[3] # input ids\n",
    "\n",
    "input_sent2_2 = inputs[4] # attention masks\n",
    "\n",
    "input_sent2_3 = inputs[5] # token type ids (of the 2nd sentences)\n",
    "\n",
    "input_doc_1 = inputs[15]\n",
    "\n",
    "input_doc_2 = inputs[16]\n",
    "\n",
    "input_doc_3 = inputs[17] # token type ids of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8fbb3",
   "metadata": {},
   "source": [
    "notes\n",
    "- dropout rate is not specified (also should different dropout layers have different dropout rates?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6946a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8bce0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = [input_sent1_1, input_sent1_2, input_sent1_3,\n",
    "                input_sent2_1, input_sent2_2, input_sent2_3,\n",
    "                input_sent3_1, input_sent3_2, input_sent3_3,\n",
    "                input_sent4_1, input_sent4_2, input_sent4_3,\n",
    "                input_sent5_1, input_sent5_2, input_sent5_3,\n",
    "                input_doc_1, input_doc_2, input_doc_3]\n",
    "model = Model(inputs=model_inputs, outputs=[yhat], name=\"keras_func_model\") ########### HELP ###########\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics = ['accuracy']) # TO BE CHANGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c517ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 297s 14s/step - loss: 0.7175 - accuracy: 0.5200\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 106s 11s/step - loss: 0.7958 - accuracy: 0.4200\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 106s 12s/step - loss: 0.7477 - accuracy: 0.4800\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 106s 12s/step - loss: 0.8235 - accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 115s 13s/step - loss: 0.7444 - accuracy: 0.5200\n"
     ]
    }
   ],
   "source": [
    "input_dict = {'input_ii_sent1': inputs[0], 'input_am_sent1': inputs[1], 'input_tti_sent1': inputs[2],\n",
    "              'input_ii_sent2': inputs[3], 'input_am_sent2': inputs[4], 'input_tti_sent2': inputs[5],\n",
    "              'input_ii_sent3': inputs[6], 'input_am_sent3': inputs[7], 'input_tti_sent3': inputs[8],\n",
    "              'input_ii_sent4': inputs[9], 'input_am_sent4': inputs[10], 'input_tti_sent4': inputs[11],\n",
    "              'input_ii_sent5': inputs[12], 'input_am_sent5': inputs[13], 'input_tti_sent5': inputs[14],\n",
    "              'input_ii_doc': inputs[15], 'input_am_doc': inputs[16], 'input_tti_doc': inputs[17],\n",
    "}\n",
    "history = model.fit(input_dict, df_train['humor'], epochs=5, batch_size=6) # dropout 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e216efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = model.fit(input_dict, df_train['humor'], epochs=5, batch_size=6) # dropout 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4df0c2af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 257s 15s/step - loss: 0.7808 - accuracy: 0.4200\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 135s 15s/step - loss: 0.9668 - accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 133s 15s/step - loss: 0.7824 - accuracy: 0.5800\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 122s 13s/step - loss: 0.9730 - accuracy: 0.4000\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 131s 15s/step - loss: 0.8641 - accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(input_dict, df_train['humor'], epochs=5, batch_size=6) # dropout 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "257bae4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 96s 10s/step - loss: 0.7491 - accuracy: 0.4800\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 97s 11s/step - loss: 0.8369 - accuracy: 0.3800\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 96s 11s/step - loss: 0.7065 - accuracy: 0.5600\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 96s 10s/step - loss: 0.7860 - accuracy: 0.4400\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 99s 11s/step - loss: 0.7077 - accuracy: 0.5400\n"
     ]
    }
   ],
   "source": [
    "history3 = model.fit(input_dict, df_train['humor'], epochs=5, batch_size=6) # dropout 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "12175478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 89s 10s/step - loss: 0.8301 - accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 95s 10s/step - loss: 0.7218 - accuracy: 0.5200\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 97s 11s/step - loss: 0.7228 - accuracy: 0.6200\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 91s 10s/step - loss: 0.7359 - accuracy: 0.3600\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 91s 10s/step - loss: 0.7744 - accuracy: 0.5600\n"
     ]
    }
   ],
   "source": [
    "history4 = model.fit(input_dict, df_train['humor'], epochs=5, batch_size=6) # dropout 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfe12180",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "764d2715",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy w/ 0.2 dropout\n",
    "sklearn.metrics.accuracy_score(y_test.iloc[:50,0], (test_preds>0.5).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f55ede36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy w/ 0.5 dropout\n",
    "test_preds2 = model.predict(test_inputs)\n",
    "sklearn.metrics.accuracy_score(y_test.iloc[:50,0], (test_preds2>0.5).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aeb2d1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy w/ 0.7 dropout\n",
    "test_preds3 = model.predict(test_inputs)\n",
    "sklearn.metrics.accuracy_score(y_test.iloc[:50,0], (test_preds3>0.5).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ebb4fbcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy w/ 0.9 dropout\n",
    "test_preds4 = model.predict(test_inputs)\n",
    "sklearn.metrics.accuracy_score(y_test.iloc[:50,0], (test_preds4>0.5).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "35da32a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== \n",
      "mean_absolute_error  : 0.09999999999999999\n",
      "mean_squared_error  : 0.009999999999999998\n",
      "r2 score  : 0.96\n",
      "================== \n",
      "f1_score  : 0.6666666666666666\n",
      "[[0 1]\n",
      " [0 1]]\n",
      "Acc 0.5 Prec 0.5 Rec 1.0 F1 0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### THEIR CODE ######\n",
    "# Evaluation Metrics\n",
    "import sklearn\n",
    "def print_evaluation_metrics(y_true, y_pred, label='', is_regression=True, label2=''):\n",
    "    print('==================', label2)\n",
    "    ### For regression\n",
    "    if is_regression:\n",
    "        print('mean_absolute_error',label,':', sklearn.metrics.mean_absolute_error(y_true, y_pred))\n",
    "        print('mean_squared_error',label,':', sklearn.metrics.mean_squared_error(y_true, y_pred))\n",
    "        print('r2 score',label,':', sklearn.metrics.r2_score(y_true, y_pred))\n",
    "        #     print('max_error',label,':', sklearn.metrics.max_error(y_true, y_pred))\n",
    "        return sklearn.metrics.mean_squared_error(y_true, y_pred)\n",
    "    else:\n",
    "        ### FOR Classification\n",
    "#         print('balanced_accuracy_score',label,':', sklearn.metrics.balanced_accuracy_score(y_true, y_pred))\n",
    "#         print('average_precision_score',label,':', sklearn.metrics.average_precision_score(y_true, y_pred))\n",
    "#         print('balanced_accuracy_score',label,':', sklearn.metrics.balanced_accuracy_score(y_true, y_pred))\n",
    "#         print('accuracy_score',label,':', sklearn.metrics.accuracy_score(y_true, y_pred))\n",
    "        print('f1_score',label,':', sklearn.metrics.f1_score(y_true, y_pred))\n",
    "        \n",
    "        matrix = sklearn.metrics.confusion_matrix(y_true, y_pred)\n",
    "        print(matrix)\n",
    "        TP,TN,FP,FN = matrix[1][1],matrix[0][0],matrix[0][1],matrix[1][0]\n",
    "        Accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
    "        Precision = TP/(TP+FP)\n",
    "        Recall = TP/(TP+FN)\n",
    "        F1 = 2*(Recall * Precision) / (Recall + Precision)\n",
    "        print('Acc', Accuracy, 'Prec', Precision, 'Rec', Recall, 'F1',F1)\n",
    "        return sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "print_evaluation_metrics([1,0], [0.9,0.1], '', True)\n",
    "print_evaluation_metrics([1,0], [1,1], '', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "654d4e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================== \n",
      "f1_score  : 0.0\n",
      "[[26  0]\n",
      " [24  0]]\n",
      "Acc 0.52 Prec nan Rec 0.0 F1 nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meerw\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_evaluation_metrics(y_test.iloc[:50,0], (test_preds2>0.5).flatten(), '', False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

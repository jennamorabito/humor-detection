{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01da05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "#!pip install torch==1.4.0\n",
    "#!pip install sentencepiece\n",
    "!pip install --upgrade transformers==3.0.2 # the authors probably used version 3.0.2\n",
    "# !pip install contractions\n",
    "# !pip install unidecode\n",
    "# !pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086fb744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\meerw\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "3.0.2\n"
     ]
    }
   ],
   "source": [
    "# the basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import logging\n",
    "# import random\n",
    "\n",
    "# data cleaning\n",
    "# import re\n",
    "# import contractions as ct\n",
    "# import string\n",
    "# import unidecode \n",
    "\n",
    "# function for computing tokenized inputs from our own module\n",
    "from bertembeddings import compute_input_arrays\n",
    "\n",
    "# math + machine learning\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from tqdm import tqdm # for nice progress meters\n",
    "import sklearn\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "import torch\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "# import bert_tokenization as tokenization\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras \n",
    "# import keras model and layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Concatenate\n",
    "# import tensorflow.keras.utils.Sequence\n",
    "# from transformers import *\n",
    "import transformers\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "# from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(tf.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6d8e5",
   "metadata": {},
   "source": [
    "# Read and split up dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18615f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('clean_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba6e35ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    100000\n",
       "True     100000\n",
       "Name: humor, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# has the right amount of jokes & non-jokes\n",
    "data.humor.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e0f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test data\n",
    "x_train, x_test = data['text'][:160000], data['text'][160000:]\n",
    "y_train, y_test = data['humor'][:160000], data['humor'][160000:]\n",
    "\n",
    "# cast back into dataframes \n",
    "x_train = x_train.to_frame('text')\n",
    "x_test = x_test.to_frame('text')\n",
    "y_train = y_train.to_frame('humor')\n",
    "y_test = y_test.to_frame('humor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c835875",
   "metadata": {},
   "source": [
    "# Tokenize inputs to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f45d98a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_npz(filename, arr):\n",
    "    \"\"\"\n",
    "    arr: list of 2D arrays\n",
    "    \"\"\"\n",
    "    if '.npz' not in filename:\n",
    "        filename += '.npz'\n",
    "    arr_dict = dict(zip(map(str, range(len(arr))), arr))\n",
    "    np.savez_compressed(filename, **arr_dict)\n",
    "    \n",
    "def load_npz(filename):\n",
    "    if '.npz' not in filename:\n",
    "        filename += '.npz'\n",
    "    return np.load(filename, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57bdebb7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "1it [00:08,  8.51s/it]Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "2it [00:16,  7.96s/it]Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "3it [00:24,  8.13s/it]Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "4it [00:32,  8.28s/it]Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "5it [00:41,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# only tokenize BATCH_SIZE examples in train and test data at a time\n",
    "BATCH_SIZE = 50\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "inputs = compute_input_arrays(x_train[:BATCH_SIZE], ['text'], tokenizer)\n",
    "save_npz(\"testrun_model_train_inputs.npz\", inputs)\n",
    "test_inputs = compute_input_arrays(x_test[:BATCH_SIZE], ['text'], tokenizer)\n",
    "save_npz(\"testrun_model_test_inputs.npz\", test_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e949be",
   "metadata": {},
   "source": [
    "# ColBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0f977d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# model architecture\n",
    "SENT_INPUT_LEN = 20\n",
    "DOC_INPUT_LEN = 100\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "# 18 inputs, 3 for each parallel path (5 sentence-level paths & 1 document-level path)\n",
    "input_sent1_1 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_ii_sent1') # input IDs\n",
    "input_sent1_2 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_am_sent1') # attention masks\n",
    "input_sent1_3 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_tti_sent1') # token type IDs\n",
    "\n",
    "input_sent2_1 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_ii_sent2')\n",
    "input_sent2_2 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_am_sent2')\n",
    "input_sent2_3 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_tti_sent2')\n",
    "\n",
    "input_sent3_1 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_ii_sent3')\n",
    "input_sent3_2 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_am_sent3')\n",
    "input_sent3_3 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_tti_sent3')\n",
    "\n",
    "input_sent4_1 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_ii_sent4')\n",
    "input_sent4_2 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_am_sent4')\n",
    "input_sent4_3 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_tti_sent4')\n",
    "\n",
    "input_sent5_1 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_ii_sent5')\n",
    "input_sent5_2 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_am_sent5')\n",
    "input_sent5_3 = Input(shape=(SENT_INPUT_LEN,), dtype=tf.int32, name='input_tti_sent5')\n",
    "\n",
    "input_doc_1 = Input(shape=(DOC_INPUT_LEN,), dtype=tf.int32, name='input_ii_doc')\n",
    "input_doc_2 = Input(shape=(DOC_INPUT_LEN,), dtype=tf.int32, name='input_am_doc')\n",
    "input_doc_3 = Input(shape=(DOC_INPUT_LEN,), dtype=tf.int32, name='input_tti_doc')\n",
    "\n",
    "# embedding layer for sentences and documents\n",
    "#bert_embeddings = Embedding(num_tokens,embedding_dim,embeddings_initializer=keras.initializers.Constant(embedding_matrix),trainable=False)\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "#bert_embeddings = bert_model(input_ids=input_sent1_1, attention_mask=input_sent1_2, token_type_ids=input_sent1_3) ########### HELP ###########\n",
    "bert_embeddings1 = bert_model(input_sent1_1, attention_mask=input_sent1_2, token_type_ids=input_sent1_3)\n",
    "bert_embeddings2 = bert_model(input_sent2_1, attention_mask=input_sent2_2, token_type_ids=input_sent2_3)\n",
    "bert_embeddings3 = bert_model(input_sent3_1, attention_mask=input_sent3_2, token_type_ids=input_sent3_3)\n",
    "bert_embeddings4 = bert_model(input_sent4_1, attention_mask=input_sent4_2, token_type_ids=input_sent4_3)\n",
    "bert_embeddings5 = bert_model(input_sent5_1, attention_mask=input_sent5_2, token_type_ids=input_sent5_3)\n",
    "bert_embeddings6 = bert_model(input_doc_1, attention_mask=input_doc_2, token_type_ids=input_doc_3)\n",
    "\n",
    "# get pooled vectors of BERT sentence embeddings\n",
    "x1 = bert_embeddings1[1] # can also do GlobalAveragePooling1D()\n",
    "x2 = bert_embeddings2[1] \n",
    "x3 = bert_embeddings3[1]\n",
    "x4 = bert_embeddings4[1]\n",
    "x5 = bert_embeddings5[1]\n",
    "x6 = bert_embeddings6[1] \n",
    "\n",
    "# fully connected layer w/ dropout\n",
    "h1_1 = Dense(32, activation='relu', name=\"hidden1_sent1\")(x1)\n",
    "h1_2 = Dense(32, activation='relu', name=\"hidden1_sent2\")(x2)\n",
    "h1_3 = Dense(32, activation='relu', name=\"hidden1_sent3\")(x3)\n",
    "h1_4 = Dense(32, activation='relu', name=\"hidden1_sent4\")(x4)\n",
    "h1_5 = Dense(32, activation='relu', name=\"hidden1_sent5\")(x5)\n",
    "h1_6 = Dense(256, activation='relu', name=\"hidden1_doc\")(x6)\n",
    "\n",
    "h1_dropout1 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent1\")(h1_1) ####################################################\n",
    "h1_dropout2 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent2\")(h1_2) ####################################################\n",
    "h1_dropout3 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent3\")(h1_3) #                rate TO BE CHANGED                # \n",
    "h1_dropout4 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent4\")(h1_4) #                                                  #\n",
    "h1_dropout5 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent5\")(h1_5) ####################################################\n",
    "h1_dropout6 = Dropout(DROPOUT_RATE, name=\"h1_dropout_doc\")(h1_6)   ####################################################\n",
    "\n",
    "# fully connected layer\n",
    "h2_1 = Dense(8, activation='relu', name=\"hidden2_sent1\")(h1_dropout1)\n",
    "h2_2 = Dense(8, activation='relu', name=\"hidden2_sent2\")(h1_dropout2)\n",
    "h2_3 = Dense(8, activation='relu', name=\"hidden2_sent3\")(h1_dropout3)\n",
    "h2_4 = Dense(8, activation='relu', name=\"hidden2_sent4\")(h1_dropout4)\n",
    "h2_5 = Dense(8, activation='relu', name=\"hidden2_sent5\")(h1_dropout5)\n",
    "h2_6 = Dense(64, activation='relu', name=\"hidden2_doc\")(h1_dropout6)\n",
    "\n",
    "# concatenate outputs of all 6 parallel layers\n",
    "xx = Concatenate()([h2_1, h2_2, h2_3, h2_4, h2_5, h2_6])\n",
    "\n",
    "# fully connected layer w/ dropout for concatenated inputs\n",
    "h3 = Dense(512, activation='relu', name=\"hidden3\")(xx)\n",
    "h3_dropout = Dropout(DROPOUT_RATE)(h3) ################ rate TO BE CHANGED ################\n",
    "\n",
    "# fully connected layer\n",
    "h4 = Dense(256, activation='relu', name=\"hidden4\")(h3_dropout)\n",
    "\n",
    "# final output layer\n",
    "yhat = Dense(1, activation='sigmoid', name=\"output\")(h4) # need to figure out dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8bce0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and compile model\n",
    "model_inputs = [input_sent1_1, input_sent1_2, input_sent1_3,\n",
    "                input_sent2_1, input_sent2_2, input_sent2_3,\n",
    "                input_sent3_1, input_sent3_2, input_sent3_3,\n",
    "                input_sent4_1, input_sent4_2, input_sent4_3,\n",
    "                input_sent5_1, input_sent5_2, input_sent5_3,\n",
    "                input_doc_1, input_doc_2, input_doc_3]\n",
    "model = Model(inputs=model_inputs, outputs=[yhat], name=\"keras_func_model\") ########### HELP ###########\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics = ['accuracy']) # TO BE CHANGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c517ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 297s 14s/step - loss: 0.7175 - accuracy: 0.5200\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 106s 11s/step - loss: 0.7958 - accuracy: 0.4200\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 106s 12s/step - loss: 0.7477 - accuracy: 0.4800\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 106s 12s/step - loss: 0.8235 - accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 115s 13s/step - loss: 0.7444 - accuracy: 0.5200\n"
     ]
    }
   ],
   "source": [
    "# train model using 0.2 dropout\n",
    "input_dict = {'input_ii_sent1': inputs[0], 'input_am_sent1': inputs[1], 'input_tti_sent1': inputs[2],\n",
    "              'input_ii_sent2': inputs[3], 'input_am_sent2': inputs[4], 'input_tti_sent2': inputs[5],\n",
    "              'input_ii_sent3': inputs[6], 'input_am_sent3': inputs[7], 'input_tti_sent3': inputs[8],\n",
    "              'input_ii_sent4': inputs[9], 'input_am_sent4': inputs[10], 'input_tti_sent4': inputs[11],\n",
    "              'input_ii_sent5': inputs[12], 'input_am_sent5': inputs[13], 'input_tti_sent5': inputs[14],\n",
    "              'input_ii_doc': inputs[15], 'input_am_doc': inputs[16], 'input_tti_doc': inputs[17],\n",
    "}\n",
    "history = model.fit(input_dict, df_train['humor'], epochs=5, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "af703c90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 257s 15s/step - loss: 0.7808 - accuracy: 0.4200\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 135s 15s/step - loss: 0.9668 - accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 133s 15s/step - loss: 0.7824 - accuracy: 0.5800\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 122s 13s/step - loss: 0.9730 - accuracy: 0.4000\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 131s 15s/step - loss: 0.8641 - accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "# train model using 0.5 dropout\n",
    "history2 = model.fit(input_dict, df_train['humor'], epochs=5, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "38d484ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 96s 10s/step - loss: 0.7491 - accuracy: 0.4800\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 97s 11s/step - loss: 0.8369 - accuracy: 0.3800\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 96s 11s/step - loss: 0.7065 - accuracy: 0.5600\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 96s 10s/step - loss: 0.7860 - accuracy: 0.4400\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 99s 11s/step - loss: 0.7077 - accuracy: 0.5400\n"
     ]
    }
   ],
   "source": [
    "# train model using 0.7 dropout\n",
    "history3 = model.fit(input_dict, df_train['humor'], epochs=5, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2d20775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "9/9 [==============================] - 89s 10s/step - loss: 0.8301 - accuracy: 0.5000\n",
      "Epoch 2/5\n",
      "9/9 [==============================] - 95s 10s/step - loss: 0.7218 - accuracy: 0.5200\n",
      "Epoch 3/5\n",
      "9/9 [==============================] - 97s 11s/step - loss: 0.7228 - accuracy: 0.6200\n",
      "Epoch 4/5\n",
      "9/9 [==============================] - 91s 10s/step - loss: 0.7359 - accuracy: 0.3600\n",
      "Epoch 5/5\n",
      "9/9 [==============================] - 91s 10s/step - loss: 0.7744 - accuracy: 0.5600\n"
     ]
    }
   ],
   "source": [
    "# train model using 0.9 dropout\n",
    "history4 = model.fit(input_dict, df_train['humor'], epochs=5, batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f615c7",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "23be26ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy w/ 0.2 dropout\n",
    "test_preds = model.predict(test_inputs)\n",
    "sklearn.metrics.accuracy_score(y_test.iloc[:50,0], (test_preds>0.5).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "30e0048c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy w/ 0.5 dropout\n",
    "test_preds2 = model.predict(test_inputs)\n",
    "sklearn.metrics.accuracy_score(y_test.iloc[:50,0], (test_preds2>0.5).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dcbe2d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy w/ 0.7 dropout\n",
    "test_preds3 = model.predict(test_inputs)\n",
    "sklearn.metrics.accuracy_score(y_test.iloc[:50,0], (test_preds3>0.5).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "806e6abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test accuracy w/ 0.9 dropout\n",
    "test_preds4 = model.predict(test_inputs)\n",
    "sklearn.metrics.accuracy_score(y_test.iloc[:50,0], (test_preds4>0.5).flatten())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

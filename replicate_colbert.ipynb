{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086fb744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb6d8e5",
   "metadata": {},
   "source": [
    "### Check if data has the same distribution as their final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18615f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba6e35ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    100000\n",
       "True     100000\n",
       "Name: humor, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# has the right amount of jokes & non-jokes\n",
    "data.humor.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5e0f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test data\n",
    "x_train, x_test = data['text'][:160000], data['text'][160000:]\n",
    "y_train, y_test = data['humor'][:160000], data['humor'][160000:]\n",
    "\n",
    "# cast back into dataframes \n",
    "x_train = x_train.to_frame('text')\n",
    "x_test = x_test.to_frame('text')\n",
    "y_train = y_train.to_frame('humor')\n",
    "y_test = y_test.to_frame('humor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fad6482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Me: my cat isn't overweight; she's just big-boned vet: this is a dog</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Did you hear, john wayne bobbit got his penis cut off again? isn't that redickless?</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jesus walks into a bar no he didn't, because he isn't real.</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Video breaks down why machismo isn't synonymous with latino men</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>George clooney's ex-anchorman dad isn't having it with sinclair</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Miss russia 'haters' say elmira abdrazakova isn't russian enough (photos)</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>It's so cool how math isn't real now that i'm a grown up.</th>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>There's a shockingly high chance the seafood you're eating isn't legit</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This is what happens 'when mama isn't home'</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wells fargo doesn't want you to know its scandal isn't hurting profits</th>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>924 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    humor\n",
       "text                                                     \n",
       "Me: my cat isn't overweight; she's just big-bon...   True\n",
       "Did you hear, john wayne bobbit got his penis c...   True\n",
       "Jesus walks into a bar no he didn't, because he...   True\n",
       "Video breaks down why machismo isn't synonymous...  False\n",
       "George clooney's ex-anchorman dad isn't having ...  False\n",
       "...                                                   ...\n",
       "Miss russia 'haters' say elmira abdrazakova isn...  False\n",
       "It's so cool how math isn't real now that i'm a...   True\n",
       "There's a shockingly high chance the seafood yo...  False\n",
       "This is what happens 'when mama isn't home'         False\n",
       "Wells fargo doesn't want you to know its scanda...  False\n",
       "\n",
       "[924 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# haven't done processing for special words (eg isn't --> is not)\n",
    "data[data['text'].str.contains(\"isn't\")].set_index('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31bb223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ' '.join(data['text'])\n",
    "all_puncs = re.findall(r'[^\\w\\s]', all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3145244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_puncs = []\n",
    "for t in data['text']:\n",
    "    num_puncs = re.findall(r'[^\\w\\s]', t)\n",
    "    all_puncs.append(num_puncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a3e4cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_puncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abbb3c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-382f69dcab46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#!pip install contractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hi I'm isn't\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "# if we need to expand contractions ourselves\n",
    "\n",
    "#!pip install contractions\n",
    "import contractions as ct\n",
    "ct.fix(\"Hi I'm isn't\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e949be",
   "metadata": {},
   "source": [
    "## ColBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras\n",
    "#!pip install tensorflow\n",
    "#!pip install --upgrade --user transformers==3.0.2 # the authors probably used version 3.0.2\n",
    "#!pip install transformers\n",
    "#!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b57749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n",
      "3.0.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "# import bert_tokenization as tokenization\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras \n",
    "\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "# from transformers import *\n",
    "import transformers\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re    #for regex\n",
    "\n",
    "# import keras model layers\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Concatenate\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(tf.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f977d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The first argument to `Layer.call` must always be passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-aa0807940a0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mbert_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTFBertModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m#bert_embeddings = bert_model(input_ids=input_sent1_1, attention_mask=input_sent1_2, token_type_ids=input_sent1_3) ########### HELP ###########\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mbert_embeddings1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_sent1_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_sent1_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_sent1_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mbert_embeddings2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_sent2_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_sent2_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_sent2_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mbert_embeddings3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_sent3_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_sent3_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_sent3_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_split_out_first_arg\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3093\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3094\u001b[0m       raise ValueError(\n\u001b[1;32m-> 3095\u001b[1;33m           'The first argument to `Layer.call` must always be passed.')\n\u001b[0m\u001b[0;32m   3096\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3097\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The first argument to `Layer.call` must always be passed."
     ]
    }
   ],
   "source": [
    "SENT_INPUT_LEN = 20\n",
    "DOC_INPUT_LEN = 100\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "# 18 inputs, 3 for each parallel path (5 sentence-level paths & 1 document-level path)\n",
    "input_sent1_1 = Input(shape=(SENT_INPUT_LEN,), name='input_ii_sent1') # input IDs\n",
    "input_sent1_2 = Input(shape=(SENT_INPUT_LEN,), name='input_am_sent1') # attention masks\n",
    "input_sent1_3 = Input(shape=(SENT_INPUT_LEN,), name='input_tti_sent1') # token type IDs\n",
    "\n",
    "input_sent2_1 = Input(shape=(SENT_INPUT_LEN,), name='input_ii_sent2')\n",
    "input_sent2_2 = Input(shape=(SENT_INPUT_LEN,), name='input_am_sent2')\n",
    "input_sent2_3 = Input(shape=(SENT_INPUT_LEN,), name='input_tti_sent2')\n",
    "\n",
    "input_sent3_1 = Input(shape=(SENT_INPUT_LEN,), name='input_ii_sent3')\n",
    "input_sent3_2 = Input(shape=(SENT_INPUT_LEN,), name='input_am_sent3')\n",
    "input_sent3_3 = Input(shape=(SENT_INPUT_LEN,), name='input_tti_sent3')\n",
    "\n",
    "input_sent4_1 = Input(shape=(SENT_INPUT_LEN,), name='input_ii_sent4')\n",
    "input_sent4_2 = Input(shape=(SENT_INPUT_LEN,), name='input_am_sent4')\n",
    "input_sent4_3 = Input(shape=(SENT_INPUT_LEN,), name='input_tti_sent4')\n",
    "\n",
    "input_sent5_1 = Input(shape=(SENT_INPUT_LEN,), name='input_ii_sent5')\n",
    "input_sent5_2 = Input(shape=(SENT_INPUT_LEN,), name='input_am_sent5')\n",
    "input_sent5_3 = Input(shape=(SENT_INPUT_LEN,), name='input_tti_sent5')\n",
    "\n",
    "input_doc_1 = Input(shape=(DOC_INPUT_LEN,), name='input_ii_doc')\n",
    "input_doc_2 = Input(shape=(DOC_INPUT_LEN,), name='input_am_doc')\n",
    "input_doc_3 = Input(shape=(DOC_INPUT_LEN,), name='input_tti_doc')\n",
    "\n",
    "# embedding layer for sentences and documents\n",
    "#bert_embeddings = Embedding(num_tokens,embedding_dim,embeddings_initializer=keras.initializers.Constant(embedding_matrix),trainable=False)\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "#bert_embeddings = bert_model(input_ids=input_sent1_1, attention_mask=input_sent1_2, token_type_ids=input_sent1_3) ########### HELP ###########\n",
    "bert_embeddings1 = bert_model(input_ids=input_sent1_1, attention_mask=input_sent1_2, token_type_ids=input_sent1_3)\n",
    "bert_embeddings2 = bert_model(input_ids=input_sent2_1, attention_mask=input_sent2_2, token_type_ids=input_sent2_3)\n",
    "bert_embeddings3 = bert_model(input_ids=input_sent3_1, attention_mask=input_sent3_2, token_type_ids=input_sent3_3)\n",
    "bert_embeddings4 = bert_model(input_ids=input_sent4_1, attention_mask=input_sent4_2, token_type_ids=input_sent4_3)\n",
    "bert_embeddings5 = bert_model(input_ids=input_sent5_1, attention_mask=input_sent5_2, token_type_ids=input_sent5_3)\n",
    "bert_embeddings6 = bert_model(input_ids=input_sent6_1, attention_mask=input_sent6_2, token_type_ids=input_sent6_3)\n",
    "\n",
    "# get pooled vectors of BERT sentence embeddings\n",
    "#x1 = bert_embeddings[0]['pooled_vector'] # can also do GlobalAveragePooling1D()\n",
    "#x2 = bert_embeddings[1]['pooled_vector']\n",
    "#x3 = bert_embeddings[2]['pooled_vector']\n",
    "#x4 = bert_embeddings[3]['pooled_vector']\n",
    "#x5 = bert_embeddings[4]['pooled_vector']\n",
    "#x6 = bert_embeddings[5]['pooled_vector']\n",
    "x1 = bert_embeddings1[1] \n",
    "x2 = bert_embeddings2[1] \n",
    "x3 = bert_embeddings3[1]\n",
    "x4 = bert_embeddings4[1]\n",
    "x5 = bert_embeddings5[1]\n",
    "x6 = bert_embeddings6[1] \n",
    "\n",
    "# fully connected layer w/ dropout\n",
    "h1_1 = Dense(32, activation='relu', name=\"hidden1_sent1\")(x1)\n",
    "h1_2 = Dense(32, activation='relu', name=\"hidden1_sent2\")(x2)\n",
    "h1_3 = Dense(32, activation='relu', name=\"hidden1_sent3\")(x3)\n",
    "h1_4 = Dense(32, activation='relu', name=\"hidden1_sent4\")(x4)\n",
    "h1_5 = Dense(32, activation='relu', name=\"hidden1_sent5\")(x5)\n",
    "h1_6 = Dense(256, activation='relu', name=\"hidden1_doc\")(x6)\n",
    "\n",
    "h1_dropout1 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent1\")(h1_1) ####################################################\n",
    "h1_dropout2 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent2\")(h1_2) ####################################################\n",
    "h1_dropout3 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent3\")(h1_3) #                rate TO BE CHANGED                # \n",
    "h1_dropout4 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent4\")(h1_4) #                                                  #\n",
    "h1_dropout5 = Dropout(DROPOUT_RATE, name=\"h1_dropout_sent5\")(h1_5) ####################################################\n",
    "h1_dropout6 = Dropout(DROPOUT_RATE, name=\"h1_dropout_doc\")(h1_6)   ####################################################\n",
    "\n",
    "# fully connected layer\n",
    "h2_1 = Dense(8, activation='relu', name=\"hidden2_sent1\")(h1_dropout1)\n",
    "h2_2 = Dense(8, activation='relu', name=\"hidden2_sent2\")(h1_dropout2)\n",
    "h2_3 = Dense(8, activation='relu', name=\"hidden2_sent3\")(h1_dropout3)\n",
    "h2_4 = Dense(8, activation='relu', name=\"hidden2_sent4\")(h1_dropout4)\n",
    "h2_5 = Dense(8, activation='relu', name=\"hidden2_sent5\")(h1_dropout5)\n",
    "h2_6 = Dense(64, activation='relu', name=\"hidden2_doc\")(h1_dropout6)\n",
    "\n",
    "# concatenate outputs of all 6 parallel layers\n",
    "xx = Concatenate()([h2_1, h2_2, h2_3, h2_4, h2_5, h2_6])\n",
    "\n",
    "# fully connected layer w/ dropout for concatenated inputs\n",
    "h3 = Dense(512, activation='relu', name=\"hidden3\")(xx)\n",
    "h3_dropout = Dropout(DROPOUT_RATE)(h3) ################ rate TO BE CHANGED ################\n",
    "\n",
    "# fully connected layer\n",
    "h4 = Dense(256, activation='relu', name=\"hidden4\")(h3_dropout)\n",
    "\n",
    "# final output layer\n",
    "yhat = Dense(1, activation='sigmoid', name=\"output\")(h4) # need to figure out dropout rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d151e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Joe biden rules out 2020 bid: 'guys, i'm not r...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Watch: darvish gave hitter whiplash with slow ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you call a turtle without its shell? d...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  Joe biden rules out 2020 bid: 'guys, i'm not r...  False\n",
       "1  Watch: darvish gave hitter whiplash with slow ...  False\n",
       "2  What do you call a turtle without its shell? d...   True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>humor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What kind of cat should you take into the  des...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remember when people used to have to be in sha...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pizza is always good. - everyone we'll see abo...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  humor\n",
       "0  What kind of cat should you take into the  des...   True\n",
       "1  Remember when people used to have to be in sha...   True\n",
       "2  Pizza is always good. - everyone we'll see abo...   True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df = pd.read_csv('/kaggle/input/200k-short-texts-for-humor-detection/dataset.csv')\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "#df_train = pd.read_csv('/kaggle/input/200k-short-texts-for-humor-detection/train.csv')\n",
    "df_train = pd.read_csv('train.csv')\n",
    "display(df_train.head(3))\n",
    "df_train = df_train[:1000]\n",
    "\n",
    "#df_test = pd.read_csv('/kaggle/input/200k-short-texts-for-humor-detection/dev.csv')\n",
    "df_test = pd.read_csv('dev.csv')\n",
    "display(df_test.head(3))\n",
    "df_test = df_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb638a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertembeddings import compute_input_arrays\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "inputs      = compute_input_arrays(df_train, ['text'], tokenizer)\n",
    "test_inputs = compute_input_arrays(df_test, ['text'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8151bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4178it [30:50:28,  7.29s/it]   "
     ]
    }
   ],
   "source": [
    "from bertembeddings import compute_input_arrays\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#inputs      = compute_input_arrays(x_train, ['text'], tokenizer)\n",
    "#test_inputs = compute_input_arrays(x_test, ['text'], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f3a346",
   "metadata": {},
   "source": [
    "### each sentence-level path is differentiated sentence position in the document (maximum 5 sentences in a document)\n",
    "input_sent1_1 = inputs[0] # input ids\n",
    "\n",
    "input_sent1_2 = inputs[1] # attention masks\n",
    "\n",
    "input_sent1_3 = inputs[2] # token type ids (of the first sentences)\n",
    "\n",
    "input_sent2_1 = inputs[3] # input ids\n",
    "\n",
    "input_sent2_2 = inputs[4] # attention masks\n",
    "\n",
    "input_sent2_3 = inputs[5] # token type ids (of the 2nd sentences)\n",
    "\n",
    "input_doc_1 = inputs[15]\n",
    "\n",
    "input_doc_2 = inputs[16]\n",
    "\n",
    "input_doc_3 = inputs[17] # token type ids of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f8fbb3",
   "metadata": {},
   "source": [
    "notes\n",
    "- not sure if we can just use the default axis for Concatenate (default is axis=-1)\n",
    "- dropout rate is not specified (also should different dropout layers have different dropout rates?)\n",
    "- output layer activation function is sigmoid right???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = [input_sent1_1, input_sent1_2, input_sent1_3,\n",
    "                input_sent2_1, input_sent2_2, input_sent2_3,\n",
    "                input_sent3_1, input_sent3_2, input_sent3_3,\n",
    "                input_sent4_1, input_sent4_2, input_sent4_3,\n",
    "                input_sent5_1, input_sent5_2, input_sent5_3,\n",
    "                input_doc_1, input_doc_2, input_doc_3]\n",
    "model = Model(inputs=model_inputs, outputs=yhat, name=\"keras_func_model\") ########### HELP ###########\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics = ['accuracy']) # TO BE CHANGED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5e397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dict = {'input_ii_sent1': inputs[0], 'input_am_sent1': inputs[1], 'input_tti_sent1': inputs[2],\n",
    "              'input_ii_sent2': inputs[3], 'input_am_sent2': inputs[4], 'input_tti_sent2': inputs[5],\n",
    "              'input_ii_sent3': inputs[6], 'input_am_sent3': inputs[7], 'input_tti_sent3': inputs[8],\n",
    "              'input_ii_sent4': inputs[9], 'input_am_sent4': inputs[10], 'input_tti_sent4': inputs[11],\n",
    "              'input_ii_sent5': inputs[12], 'input_am_sent5': inputs[13], 'input_tti_sent5': inputs[15],\n",
    "              'input_ii_doc': inputs[16], 'input_am_doc': inputs[17], 'input_tti_doc': inputs[18],\n",
    "}\n",
    "history = model.fit(input_dict, df_train['humor'], epochs=5, batch_size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b5036a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(test_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
